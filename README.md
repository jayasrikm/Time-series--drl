# Time-series--drl
Advanced Time Series Forecasting with Deep Reinforcement Learning (DRL) This project challenges advanced students to move beyond standard ARIMA or Prophet models by implementing a Deep Reinforcement Learning (DRL) agent to optimize a dynamic forecasting and resource allocation strategy
This Jupyter Notebook, drl.ipynb, implements a Deep Reinforcement Learning (DRL) agent using a Deep Q-Network (DQN) to solve a synthetic inventory management optimization problem. It compares the DRL agent's performance against a traditional SARIMAX-based forecasting heuristic. Project OverviewThe goal of this project is to minimize the total inventory cost over a period by determining the optimal daily order quantity. The total cost is a combination of:Holding Cost: $0.10$ per unit of inventory held.Stockout (Unmet Demand) Cost: $1.00$ per unit of unmet demand.Ordering Cost: $0.50$ per unit ordered.Setup and DependenciesThe notebook requires the following libraries, which are installed in the initial cell:gymnasium (for the custom MDP environment)stable-baselines3[extra]pmdarima, statsmodels (for the SARIMAX baseline)tensorflow/keras (for the DQN implementation)numpy, pandas, matplotlib, tqdm Data GenerationSynthetic Data: A dataset of 1200 daily demand values is generated. The demand time series includes a positive trend ($0.01$ per step), seasonality (period of $30$ days), and Gaussian noise.Splitting: The data is split into a training set (the first 900 steps) and a test set (the remaining 300 steps).Models and Methodology1. Baseline Model (SARIMAX Heuristic)A SARIMAX(1,0,1)(1,0,1,30) time series model is trained on the training data.The model forecasts the demand for each day of the test period.The ordering policy is a simple heuristic: Order = Max(0, Forecasted_Demand - Current_Inventory). The total cost is calculated based on this greedy policy.2. Deep Reinforcement Learning (DQN)Environment: A custom InventoryMDPEnv is created, which models the inventory process as a Markov Decision Process (MDP).State Space: The agent observes the current inventory level and a lookback window of the previous 5 daily demand values.Action Space: Discrete actions representing the quantity to order (from $0$ to MAX_ORDER=50).Reward: The reward is the negative of the total period cost.Constraints: Maximum inventory is capped at MAX_INV=100.Agent: A Deep Q-Network (DQN) is implemented and trained on the environment for 1000 steps using an $\epsilon$-greedy exploration strategy and a replay buffer. Results and ComparisonThe final cell compares the total cumulative cost on the test set for both policies:PolicyTotal Cumulative CostBaseline (SARIMAX Heuristic)$11,451.90$DRL Agent (DQN)$7,367.60$The DRL Agent achieved a significantly lower total cost, demonstrating its effectiveness in learning a more complex, cost-minimizing inventory policy compared to the SARIMAX-based heuristic.
