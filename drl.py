# -*- coding: utf-8 -*-
"""drl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b1x3pYukSL5TwwySLf4x7z9ZB_hYBfrq
"""

!pip install --quiet gymnasium
!pip install --quiet stable-baselines3[extra]
!pip install --quiet pmdarima matplotlib pandas statsmodels tqdm

import gymnasium as gym
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import DQN
from tqdm import tqdm

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import gym
from gym import spaces
from tqdm import tqdm
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
import json

OUTPUT_DIR = "/content/outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)
print("Output directory created:", OUTPUT_DIR)

def generate_synthetic_demand(n_steps=1200, seed=42, trend=0.01, seasonality_period=30, noise_std=6.0):
    np.random.seed(seed)
    t = np.arange(n_steps)
    demand = (
        50
        + trend * t
        + 10 * np.sin(2 * np.pi * t / seasonality_period)
        + np.random.normal(scale=noise_std, size=n_steps)
    ).clip(min=0).round().astype(int)
    dates = pd.date_range(start="2020-01-01", periods=n_steps, freq="D")
    return pd.DataFrame({"date": dates, "demand": demand})

df = generate_synthetic_demand()

train_df = df.iloc[:900].reset_index(drop=True)
test_df = df.iloc[900:].reset_index(drop=True)

train_df.to_csv(f"{OUTPUT_DIR}/train_demand.csv", index=False)
test_df.to_csv(f"{OUTPUT_DIR}/test_demand.csv", index=False)

df.head()



df.to_csv(f"{OUTPUT_DIR}/full_demand.csv", index=False)
print(f"DataFrame 'df' saved to {OUTPUT_DIR}/full_demand.csv")

plt.figure(figsize=(12,4))
plt.plot(df["date"], df["demand"])
plt.title("Synthetic Demand Dataset")
plt.xlabel("Date")
plt.ylabel("Demand")
plt.savefig(f"{OUTPUT_DIR}/synthetic_demand_plot.png")
plt.show()

class InventoryEnv(gym.Env):
    metadata = {"render.modes": ["human"]}

    def __init__(self, demand_series, lookback=7, max_inventory=400, max_order=100):
        super().__init__()
        self.demand = demand_series.reset_index(drop=True)
        self.lookback = lookback
        self.max_inventory = max_inventory
        self.max_order = max_order

        self.holding_cost = 0.1
        self.stockout_cost = 1.0
        self.order_cost = 0.5

        self.action_space = spaces.Discrete(self.max_order + 1)
        obs_low = np.zeros(self.lookback + 1, dtype=np.float32)
        obs_high = np.array([np.inf]*self.lookback + [self.max_inventory], dtype=np.float32)
        self.observation_space = spaces.Box(obs_low, obs_high)

    def reset(self):
        self.t = np.random.randint(self.lookback, len(self.demand) // 3)
        self.inventory = 100
        return self._get_obs()

    def step(self, action):
        order = int(action)
        self.inventory = min(self.max_inventory, self.inventory + order)
        demand = int(self.demand.iloc[self.t])
        sold = min(self.inventory, demand)
        unmet = max(0, demand - self.inventory)
        self.inventory -= sold

        cost = (
            self.holding_cost * self.inventory
            + self.stockout_cost * unmet
            + self.order_cost * order
        )
        reward = -cost

        self.t += 1
        done = self.t >= len(self.demand)
        return self._get_obs(), reward, done, {"cost": cost, "demand": demand, "order": order}

    def _get_obs(self):
        hist = self.demand.iloc[self.t - self.lookback:self.t].values
        return np.concatenate([hist.astype(np.float32), [self.inventory]])

!pip install 'shimmy>=0.2.1'
env = InventoryEnv(train_df["demand"])
vec_env = DummyVecEnv([lambda: env])
vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=False)

model = DQN("MlpPolicy", vec_env, verbose=1, learning_rate=1e-4)
model.learn(total_timesteps=20000)
model.save(f"{OUTPUT_DIR}/drl_model.zip")

print("Model training completed.")

def rollout(model, demand_series):
    env = InventoryEnv(demand_series)
    obs = env.reset()
    done = False
    log = []
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(action)
        log.append(info)
    return pd.DataFrame(log)

roll_df = rollout(model, test_df["demand"])
roll_df.to_csv(f"{OUTPUT_DIR}/rollout.csv", index=False)
roll_df.head()

plt.figure(figsize=(12,4))
plt.plot(test_df["demand"].values, label="Demand")
plt.plot(roll_df["order"].values, label="Orders (DRL Agent)")
plt.legend()
plt.title("DRL Agent Ordering vs Demand")
plt.savefig(f"{OUTPUT_DIR}/demand_orders_plot.png")
plt.show()

train_series = train_df["demand"]
sarima = SARIMAX(train_series, order=(1,0,1), seasonal_order=(1,0,1,30))
res = sarima.fit(disp=False)
pred = res.get_forecast(steps=len(test_df)).predicted_mean.round().clip(lower=0).astype(int)

def baseline_cost(pred_values, demand_values):
    inv = 100
    total = 0
    for i in range(len(demand_values)):
        order = max(0, pred_values[i] - inv)
        inv += order
        sold = min(inv, demand_values[i])
        unmet = max(0, demand_values[i] - inv)
        inv -= sold
        total += 0.1*inv + 1.0*unmet + 0.5*order
    return total

baseline = baseline_cost(pred.values, test_df["demand"].values)
drl_cost = roll_df["cost"].sum()

metrics = {"baseline_cost": float(baseline), "drl_cost": float(drl_cost)}
json.dump(metrics, open(f"{OUTPUT_DIR}/comparison_metrics.json", "w"))
metrics

!pip install --quiet gymnasium
!pip install --quiet stable-baselines3[extra]
!pip install --quiet pmdarima matplotlib pandas statsmodels tqdm

import gymnasium as gym
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import DQN
from tqdm import tqdm

# Run only if gymnasium or stable-baselines3 not installed
!pip install --quiet gymnasium stable-baselines3[extra] pmdarima matplotlib pandas statsmodels tqdm

# Inventory MDP environment (Gymnasium-based)
import os
import json
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, Any

OUTPUT_DIR = "/content/outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

class InventoryMDPEnv(gym.Env):
    """
    Inventory + Forecasting MDP environment.

    State (observation):
      - last_k_demands: numpy array of shape (k,) containing the k most recent demands
      - inventory_level: scalar (current on-hand inventory)
      - optional: last_action or forecast can be appended

    Action:
      - integer order quantity in [0, max_order]  (discrete)

    Reward:
      - negative of total period cost:
          reward = - (holding_cost * end_inventory
                      + stockout_cost * unmet_demand
                      + ordering_cost_fixed * (order>0 ? 1 : 0)
                      + ordering_cost_unit * order)
      (Agent learns to **maximize** reward => minimize cost)

    Transition dynamics:
      - order arrives immediately (lead time = 0). Demand is realized from provided demand_series.
      - inventory updated: inventory = min(max_inventory, inventory + order)
      - sales = min(inventory, demand), unmet = demand - sales, leftover inventory persists to next step
    """
    metadata = {"render_modes": ["human"]}

    def __init__(
        self,
        demand_series: pd.Series,
        lookback: int = 7,
        max_inventory: int = 400,
        max_order: int = 100,
        holding_cost: float = 0.1,
        stockout_cost: float = 1.0,
        order_cost_unit: float = 0.5,
        order_cost_fixed: float = 0.0,
        initial_inventory: int = 100,
        truncate_on_end: bool = True,
    ):
        super().__init__()
        self.demand = demand_series.reset_index(drop=True).astype(int)
        self.n = len(self.demand)
        self.lookback = lookback
        self.max_inventory = max_inventory
        self.max_order = max_order
        self.holding_cost = holding_cost
        self.stockout_cost = stockout_cost
        self.order_cost_unit = order_cost_unit
        self.order_cost_fixed = order_cost_fixed
        self.initial_inventory = initial_inventory
        self.truncate_on_end = truncate_on_end

        # Discrete action: 0..max_order
        self.action_space = spaces.Discrete(self.max_order + 1)

        # Observation: lookback demands + current inventory
        low = np.zeros(self.lookback + 1, dtype=np.float32)
        high = np.array([np.finfo(np.float32).max] * self.lookback + [self.max_inventory], dtype=np.float32)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)

        # Internal pointers
        self.reset()

    def _get_obs(self):
        # last `lookback` demands before time t (not including demand at t)
        start = max(0, self.t - self.lookback)
        hist = self.demand.iloc[start:self.t].values.astype(np.float32)
        if len(hist) < self.lookback:
            pad = np.zeros(self.lookback - len(hist), dtype=np.float32)
            hist = np.concatenate([pad, hist])
        obs = np.concatenate([hist, np.array([self.inventory], dtype=np.float32)])
        return obs

    def reset(self, seed: int | None = None, options: Dict[str, Any] | None = None):
        # Start at a random time that allows lookback and sufficient horizon OR at 0 for deterministic runs
        if seed is not None:
            np.random.seed(seed)
        # Choose a random start in the early part to avoid always starting at t=lookback
        min_start = self.lookback
        max_start = max(self.lookback, self.n // 4)
        self.t = np.random.randint(min_start, max_start + 1)
        self.inventory = int(self.initial_inventory)
        self.cumulative_cost = 0.0
        return self._get_obs(), {}

    def step(self, action):
        assert self.action_space.contains(action), f"Invalid action: {action}"
        order = int(action)
        # Apply order (arrives immediately)
        self.inventory = min(self.max_inventory, self.inventory + order)
        demand = int(self.demand.iloc[self.t])
        sales = min(self.inventory, demand)
        unmet = max(0, demand - self.inventory)
        self.inventory -= sales  # leftover inventory after sales

        # Costs
        holding = self.holding_cost * float(self.inventory)
        stockout = self.stockout_cost * float(unmet)
        ordering_unit = self.order_cost_unit * float(order)
        ordering_fixed = self.order_cost_fixed if order > 0 else 0.0
        period_cost = holding + stockout + ordering_unit + ordering_fixed
        reward = -period_cost

        self.cumulative_cost += period_cost
        self.t += 1
        terminated = False
        truncated = False
        if self.t >= self.n:
            # reached dataset end
            if self.truncate_on_end:
                truncated = True
            else:
                terminated = True

        obs = self._get_obs()
        info = {
            "period_cost": float(period_cost),
            "holding": float(holding),
            "stockout": float(stockout),
            "ordering_unit": float(ordering_unit),
            "ordering_fixed": float(ordering_fixed),
            "demand": int(demand),
            "order": int(order),
            "inventory": int(self.inventory),
            "cumulative_cost": float(self.cumulative_cost),
        }
        return obs, float(reward), terminated, truncated, info

    def render(self, mode="human"):
        print(f"t={self.t}, inventory={self.inventory}")

    def close(self):
        pass

# Utility: create an MDP specification dictionary and save
def save_mdp_spec(env: InventoryMDPEnv, path: str = f"{OUTPUT_DIR}/mdp_spec.json"):
    spec = {
        "state": {
            "description": "last_k_demands (length lookback) and current inventory",
            "lookback": env.lookback,
            "inventory_range": [0, env.max_inventory]
        },
        "action": {
            "type": "discrete",
            "values": list(range(env.max_order + 1)),
            "meaning": "order quantity arriving immediately"
        },
        "reward": {
            "formula": "-(holding_cost * end_inventory + stockout_cost * unmet + order_cost_unit*order + order_cost_fixed*(order>0))",
            "holding_cost": env.holding_cost,
            "stockout_cost": env.stockout_cost,
            "order_cost_unit": env.order_cost_unit,
            "order_cost_fixed": env.order_cost_fixed
        },
        "transition": "inventory += order -> demand realized -> sales=min(inventory,demand) -> inventory -= sales",
        "episode": {
            "horizon": env.n,
            "termination": "end of demand series"
        }
    }
    with open(path, "w") as f:
        json.dump(spec, f, indent=2)
    return spec

# Example small usage test (only runs when executed)
if __name__ == "__main__":
    # small synthetic demand to illustrate
    s = pd.Series([50, 60, 55, 48, 70, 65, 58, 62, 67, 59, 55, 52, 60, 61, 63])
    env = InventoryMDPEnv(s, lookback=5, max_inventory=200, max_order=50, initial_inventory=50)
    spec = save_mdp_spec(env)
    print("Saved MDP spec to:", OUTPUT_DIR + "/mdp_spec.json")
    obs, _ = env.reset()
    print("initial obs:", obs)
    ob, r, term, trunc, info = env.step(5)
    print("step result:", r, info)

# Run one deterministic episode with a simple heuristic policy (order to bring inventory up to forecast)
import pandas as pd
from collections import deque
import json

def heuristic_policy(obs, target_level=120):
    # obs = [last_k_demands..., inventory]
    current_inventory = int(obs[-1])
    order = max(0, target_level - current_inventory)
    return int(order)

def run_episode_and_save(env, policy_fn, fname=f"{OUTPUT_DIR}/mdp_trace.csv"):
    obs, _ = env.reset(seed=0)
    records = []
    done = False
    terminated = False
    truncated = False
    # Gymnasium step returns obs, reward, terminated, truncated, info
    while True:
        action = policy_fn(obs)
        obs, reward, terminated, truncated, info = env.step(action)
        row = {
            "t": env.t,
            "order": info["order"],
            "demand": info["demand"],
            "inventory_after": info["inventory"],
            "period_cost": info["period_cost"],
            "cumulative_cost": info["cumulative_cost"]
        }
        records.append(row)
        if terminated or truncated:
            break
    df_trace = pd.DataFrame(records)
    df_trace.to_csv(fname, index=False)
    print(f"Saved episode trace to {fname}")
    return df_trace

# Small example with synthetic series
if __name__ != "__main__":
    # create small synthetic series for demo
    demo_s = pd.Series((50 + 10*np.sin(np.arange(100)/10) + np.random.normal(0,3,100)).round().astype(int))
    env = InventoryMDPEnv(demo_s, lookback=7, initial_inventory=80)
    save_mdp_spec(env)
    df_trace = run_episode_and_save(env, heuristic_policy)
    df_trace.head()

print("MDP spec saved as /content/outputs/mdp_spec.json")
print("Episode trace will be saved as /content/outputs/mdp_trace.csv when you run the helper.")

# This env uses gymnasium API (reset returns obs, info; step returns obs, reward, terminated, truncated, info)
import gymnasium as gym
from gymnasium import spaces

class InventoryMDPEnv(gym.Env):
    metadata = {"render_modes": ["human"]}
    def __init__(self, demand_series, lookback=7, max_inventory=400, max_order=100,
                 holding_cost=0.1, stockout_cost=1.0, order_cost_unit=0.5, order_cost_fixed=0.0, initial_inventory=100):
        super().__init__()
        self.demand = demand_series.reset_index(drop=True).astype(int)
        self.n = len(self.demand)
        self.lookback = lookback
        self.max_inventory = max_inventory
        self.max_order = max_order
        self.holding_cost = holding_cost
        self.stockout_cost = stockout_cost
        self.order_cost_unit = order_cost_unit
        self.order_cost_fixed = order_cost_fixed
        self.initial_inventory = initial_inventory

        self.action_space = spaces.Discrete(self.max_order + 1)
        low = np.zeros(self.lookback + 1, dtype=np.float32)
        high = np.array([np.finfo(np.float32).max]*self.lookback + [self.max_inventory], dtype=np.float32)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)
        self.reset()

    def _get_obs(self):
        start = max(0, self.t - self.lookback)
        hist = self.demand.iloc[start:self.t].values.astype(np.float32)
        if len(hist) < self.lookback:
            hist = np.concatenate([np.zeros(self.lookback - len(hist), dtype=np.float32), hist])
        obs = np.concatenate([hist, np.array([self.inventory], dtype=np.float32)])
        return obs

    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
        min_start = self.lookback
        max_start = max(self.lookback, self.n // 4)
        self.t = np.random.randint(min_start, max_start + 1)
        self.inventory = int(self.initial_inventory)
        self.cumulative_cost = 0.0
        return self._get_obs(), {}

    def step(self, action):
        order = int(action)
        self.inventory = min(self.max_inventory, self.inventory + order)
        demand = int(self.demand.iloc[self.t])
        sales = min(self.inventory, demand)
        unmet = max(0, demand - self.inventory)
        self.inventory -= sales

        holding = self.holding_cost * float(self.inventory)
        stockout = self.stockout_cost * float(unmet)
        ordering_unit = self.order_cost_unit * float(order)
        ordering_fixed = self.order_cost_fixed if order > 0 else 0.0
        period_cost = holding + stockout + ordering_unit + ordering_fixed
        reward = -period_cost

        self.cumulative_cost += period_cost
        self.t += 1
        terminated = False
        truncated = False
        if self.t >= self.n:
            truncated = True

        obs = self._get_obs()
        info = {
            "period_cost": float(period_cost),
            "holding": float(holding),
            "stockout": float(stockout),
            "order": int(order),
            "demand": int(demand),
            "inventory": int(self.inventory),
            "cumulative_cost": float(self.cumulative_cost)
        }
        return obs, reward, terminated, truncated, info

    def render(self, mode="human"):
        print(f"t={self.t}, inventory={self.inventory}")

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses

# Replay buffer
from collections import deque
import random

class ReplayBuffer:
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    def add(self, s, a, r, s2, done):
        self.buffer.append((s, a, r, s2, done))
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        s, a, r, s2, done = map(np.array, zip(*batch))
        return s, a, r, s2, done
    def __len__(self):
        return len(self.buffer)

# Q-network (simple MLP)
def build_q_network(input_dim, action_space, hidden_sizes=[128, 128], lr=1e-3):
    inputs = layers.Input(shape=(input_dim,), dtype=tf.float32)
    x = inputs
    for h in hidden_sizes:
        x = layers.Dense(h, activation="relu")(x)
    outputs = layers.Dense(action_space, activation="linear")(x)
    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss=losses.Huber())
    return model

from tqdm import trange
# Hyperparameters (tune these)
LOOKBACK = 7
MAX_ORDER = 100
MAX_INV = 400
INITIAL_INV = 100

env = InventoryMDPEnv(train_df["demand"], lookback=LOOKBACK, max_inventory=MAX_INV, max_order=MAX_ORDER, initial_inventory=INITIAL_INV)

input_dim = LOOKBACK + 1
n_actions = env.action_space.n

# Build networks
online_net = build_q_network(input_dim, n_actions, hidden_sizes=[256, 256], lr=1e-4)
target_net = build_q_network(input_dim, n_actions, hidden_sizes=[256, 256], lr=1e-4)
target_net.set_weights(online_net.get_weights())

replay = ReplayBuffer(capacity=100000)
epsilon = 1.0
epsilon_min = 0.05
epsilon_decay = 0.9995
gamma = 0.99
batch_size = 64
train_start = 1000
target_update_freq = 1000  # steps
max_steps = 1000  # total environment steps (increase for better training)

# Diagnostics
history_rewards = []
history_losses = []
step = 0
episode = 0
state, _ = env.reset()
total_reward_ep = 0.0

pbar = trange(max_steps, desc="Training")
for step_i in pbar:
    step += 1
    # epsilon-greedy
    if np.random.rand() < epsilon:
        action = env.action_space.sample()
    else:
        qvals = online_net.predict(state.reshape(1, -1), verbose=0)[0]
        action = int(np.argmax(qvals))
    next_state, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    replay.add(state, action, reward, next_state, done)
    total_reward_ep += reward
    state = next_state

    # training
    if len(replay) >= train_start:
        s_batch, a_batch, r_batch, s2_batch, done_batch = replay.sample(batch_size)
        # predict targets
        q_next = target_net.predict(s2_batch, verbose=0)
        q_next_max = np.max(q_next, axis=1)
        target_q = r_batch + (1 - done_batch.astype(np.float32)) * gamma * q_next_max
        q_vals = online_net.predict(s_batch, verbose=0)
        # update only taken actions
        for i, a in enumerate(a_batch):
            q_vals[i, int(a)] = target_q[i]
        # train on updated q_vals as targets
        history = online_net.fit(s_batch, q_vals, epochs=1, verbose=0)
        loss = history.history['loss'][0]
        history_losses.append(loss)

    # update epsilon
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

    # update target network
    if step % target_update_freq == 0 and step > 0:
        target_net.set_weights(online_net.get_weights())

    # handle end of episode (when env truncated or terminated)
    if done:
        history_rewards.append(total_reward_ep)
        episode += 1
        state, _ = env.reset()
        total_reward_ep = 0.0

    if step_i % 500 == 0 and len(history_rewards) > 0:
        pbar.set_postfix({"eps": f"{epsilon:.3f}", "ep_avg_r": f"{np.mean(history_rewards[-20:]):.2f}", "loss": f"{np.mean(history_losses[-50:]) if history_losses else 0:.4f}"})

# Save networks and training history
online_net.save(os.path.join(OUTPUT_DIR, "dqn_tf_model.h5"))
np.savez(os.path.join(OUTPUT_DIR, "dqn_training_history.npz"), rewards=np.array(history_rewards), losses=np.array(history_losses))
print("Training finished. Episodes:", episode, "Total steps:", step)

# Load best model (we saved the online_net)
from tensorflow.keras.models import load_model
model = load_model(os.path.join(OUTPUT_DIR, "dqn_tf_model.h5"))
# Evaluate on test_df
env_test = InventoryMDPEnv(test_df["demand"], lookback=LOOKBACK, max_inventory=MAX_INV, max_order=MAX_ORDER, initial_inventory=INITIAL_INV)

def rollout_agent(model, env, deterministic=True):
    obs, _ = env.reset(seed=0)
    done = False
    records = []
    while True:
        q = model.predict(obs.reshape(1, -1), verbose=0)[0]
        action = int(np.argmax(q))
        obs, reward, terminated, truncated, info = env.step(action)
        records.append(info)
        if terminated or truncated:
            break
    return pd.DataFrame(records)

roll_df = rollout_agent(model, env_test)
roll_df.to_csv(os.path.join(OUTPUT_DIR, "drl_rollout.csv"), index=False)
print("Rollout saved:", os.path.join(OUTPUT_DIR, "drl_rollout.csv"))
roll_df.head()

import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
train_series = train_df["demand"]
sarima = SARIMAX(train_series, order=(1,0,1), seasonal_order=(1,0,1,30))
res = sarima.fit(disp=False)
n_test = len(test_df)
fc = res.get_forecast(steps=n_test)
pred_mean = np.maximum(0, fc.predicted_mean.round().astype(int)).reset_index(drop=True)

def baseline_heuristic_cost(predictions, demand_series, initial_inventory=INITIAL_INV):
    inv = initial_inventory
    total = 0.0
    for i in range(len(demand_series)):
        order = int(max(0, predictions.iloc[i] - inv))
        inv = min(MAX_INV, inv + order)
        demand = int(demand_series.iloc[i])
        sold = min(inv, demand)
        unmet = max(0, demand - inv)
        inv -= sold
        total += 0.1 * inv + 1.0 * unmet + 0.5 * order
    return total

baseline_cost = baseline_heuristic_cost(pred_mean, test_df["demand"])
drl_cost = roll_df["period_cost"].sum()
metrics = {"baseline_cost": float(baseline_cost), "drl_cost": float(drl_cost)}
json.dump(metrics, open(os.path.join(OUTPUT_DIR, "comparison_metrics.json"), "w"))
metrics

# Training diagnostics
hist = np.load(os.path.join(OUTPUT_DIR, "dqn_training_history.npz"))
rewards = hist["rewards"] if "rewards" in hist else np.array([])
losses = hist["losses"] if "losses" in hist else np.array([])

plt.figure(figsize=(8,4))
if len(rewards)>0:
    plt.plot(np.arange(len(rewards)), rewards)
    plt.title("Episode returns during training")
    plt.xlabel("Episode")
    plt.ylabel("Return")
    plt.savefig(os.path.join(OUTPUT_DIR, "rewards_plot.png"))
    plt.show()
if len(losses)>0:
    plt.figure(figsize=(8,4))
    plt.plot(np.arange(len(losses)), losses)
    plt.title("Training losses (per update)")
    plt.xlabel("Step")
    plt.ylabel("Huber loss")
    plt.savefig(os.path.join(OUTPUT_DIR, "losses_plot.png"))
    plt.show()

# Orders vs demand (single rollout)
plt.figure(figsize=(12,4))
plt.plot(test_df["demand"].values, label="demand")
plt.plot(roll_df["order"].values, label="orders (DRL agent)")
plt.legend()
plt.title("Test Demand vs DRL Agent Orders (single rollout)")
plt.savefig(os.path.join(OUTPUT_DIR, "demand_orders_plot.png"))
plt.show()